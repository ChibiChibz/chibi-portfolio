import { ProjectLayout } from '@/components/ProjectLayout'
import { Button } from '@/components/Button'
import { ArrowDownIcon } from '@/pages/index'
import Image from 'next/image'
import recognitionImage from './Recognition.png'

export const meta = {
  author: 'Chi Thanh Pham',
  company: 'Leibniz University Hanover - Visual Analytics',
  date: '2020-12-17',
  title: 'Recognition of Interpretable Gestures',
  description:
    'Classification of 49 gestures in images using machine learning Feature extraction from ObjectDetector vs Pose Estimation Keras/Tensorflow, PyTorch & scikit-learn Creating own Datasets: Web Scraping vs Manual Total accuracy 58 % Input images; output image with skeleton and prediction including the possible interpretations.',
}

export default (props) => <ProjectLayout meta={meta} {...props} />

<Image src={recognitionImage} alt="" />

Gestures are the most intuitive form of non-verbal communication that is a vital component
to the way humans express their thoughts. Within the past two decades, the growing
interest in the study of gesture recognition has greatly improved the interaction between a
human and computer. Wearable devices have been a widely used mechanism in the earlier
years of gesture recognition however interest has shifted focus towards using methods that
provide more ease of use hence aid more natural movement such as camera-based systems.
Previous research of gesture recognition has largely been centered on the hands due to its
applications in Sign Language Recognition and virtual control. However, gestures usually
involve the whole body - e.g. bowing. This paper seeks to propose a method of recognising
interpretable gestures from a camera-based system without any special devices to make it
practical in real-world applications. 
Concepts from still-image based recognition, 
skeletalbased recognition, machine learning and deep learning are explored in this paper which
provide the foundations of this projectâ€™s methodology. Two datasets of 11615 images in
total of 49 gestures used in this paper have been acquired from web scraping and manual
photography. Experiments on the datasets determine a combination of skeletal data and
random forest classification as the most accurate method with an accuracy of 58 %.

<Button href="https://github.com/ChibiChibz/Recognition-of-Interpretable-Gestures-BA-Thesis-/blob/main/README.md" target="_blank"  variant="secondary" className="group mt-6 w-full">
  Code
</Button>
<Button href="/downloads/Recognition of Interpretable Gestures - BA Chi Thanh Pham without boxes.pdf" target="_blank"  variant="secondary" className="group mt-6 w-full">
  Thesis
  <ArrowDownIcon className="h-4 w-4 stroke-zinc-400 transition group-active:stroke-zinc-600 dark:group-hover:stroke-zinc-50 dark:group-active:stroke-zinc-50" />
</Button>